\documentclass{beamer}
\usepackage[utf8]{inputenc}

\usetheme{Madrid}
\usecolortheme{default}
\useinnertheme{circles}

\definecolor{Logo1}{rgb}{0.208, 0.2865, 0.373}
\definecolor{Logo2}{rgb}{0.000, 0.674, 0.863}

\setbeamercolor*{palette primary}{bg=Logo1, fg=white}
\setbeamercolor*{palette secondary}{bg=Logo2, fg=white}
\setbeamercolor*{palette tertiary}{bg=white, fg=Logo1}
\setbeamercolor*{palette quaternary}{bg=Logo1,fg=white}
\setbeamercolor{structure}{fg=Logo1} % itemize, enumerate, etc
\setbeamercolor{section in toc}{fg=Logo1} % TOC sections

\usepackage{graphicx,animate}
%------------------------------------------------------------
%This block of code defines the information to appear in the
%Title page
\title[Linear Algebra] %optional
{Gaussian Elimination, Inverses of Matrices}

\subtitle{Lecture 1}

\author[zhangc2019@mail.sustech.edu.cn] % (optional)
{
    Zhang Ce
}

\institute[] % (optional)
{
    Department of Electrical and Electronic Engineering\\
    Southern University of Science and Technology
}

\date[2022.9.27] % (optional)
{2022.9.27}


%End of title page configuration block
%------------------------------------------------------------



%------------------------------------------------------------
%The next block of commands puts the table of contents at the
%beginning of each section and highlights the current section:

\AtBeginSection[]
{
\begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
\end{frame}
}
%------------------------------------------------------------


\begin{document}

%The next statement creates the title page.
\frame{\titlepage}


%---------------------------------------------------------
%This block of code is for the table of contents after
%the title page
\begin{frame}
\frametitle{Table of Contents}
\tableofcontents
\end{frame}
%---------------------------------------------------------

\section{Introduction to the Whole Course}

\begin{frame}{About Peer Supporting Class...}
\textbf{Peer Supporting Class} is a program created by the Student Study Centre of SUSTech. This program invites some students to teach several major courses for freshmen.

Courses involved: Linear Algebra (here), Calculus, General Physics, Java.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{arrangement.png}
\end{figure}

Come here if you want to learn more and get better grades in these courses, but to be careful with your time arrangement. Feel free to attend these classes, no scores will be graded here!

\end{frame}

\begin{frame}{QQ Group}
\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{QR.PNG}
\end{figure}
\textbf{Please scan the QR code above to enter the QQ group.}

Feel free to ask questions in that QQ group. Help others and improve yourself by participating the discussions. Some review slides will be released in QQ group before the exam, hope these materials can help you!

\end{frame}

\begin{frame}{About Me...}
\begin{itemize}
    \item \textbf{Name}: Zhang Ce
    \item \textbf{Grade}: 3
    \item \textbf{Department}: Electrical and Electronic Engineering
    \item \textbf{Major}: Communication Engineering
    \item \textbf{E-Mail}: 11910803@mail.sustech.edu.cn
    \item Get $100 (A+)$ in Linear Algebra A, Fall 2019. \\
    \vspace{3pt}
    Midterm Exam: $100/100$ \qquad Final Exam: $100+/110$
    \item Contact me directly on QQ (recommended) or email if you have problems on this course or issues about major selection.
    \item I'm not your instructor, I'm just one of your schoolmates. I will try my best to prepare and give my lectures, and I can always learn from you at the same time.
\end{itemize}

\end{frame}

\begin{frame}{About This Course...}
\begin{itemize}
    \item \textbf{Course Name}: Linear Algebra A / B
    \item \textbf{Course Code}: MA107A / MA107B
    \item \textbf{Course Category}: GR (General Education Required Course)
    \item \textbf{Class Hours}: 64 \qquad \textbf{Total Credits}: 4
    \item \textbf{Assessment}: '334' Grading System \quad $60\%$ to pass this course\\
        \vspace{5pt}
        $30\%$ Regular Performance\\
        \vspace{3pt}
        \quad - $5\%$ Attendance (lectures \& tutorials since Week 4, $0.13\%$ each)\\
        \vspace{3pt}
        \quad - $15\%$ Quiz (4 times, $3.75\%$ each)\\
        \vspace{3pt}
        \quad - $10\%$ Assignments (every week, $0.7\%$ each)\\
        \vspace{3pt}
        $30\%$ Midterm Exam \\
        \vspace{3pt}
        $40\%$ Final Exam
\end{itemize}

\end{frame}
\begin{frame}{A General Overview of Linear Algebra}
\textbf{You will learn:}
\begin{itemize}
    \item \textbf{Chapter 1}: Matrices and Gaussian Elimination
    \item \textbf{Chapter 2}: Vector Spaces
    \item \textbf{Chapter 3}: Orthogonality\\
    \vspace{4pt}
-------------------------- Midterm Exam --------------------------
    \item \textbf{Chapter 4}: Determinants
    \item \textbf{Chapter 5}: Eigenvalues and Eigenvectors
    \item \textbf{Chapter 6}: Definiteness and Singular Value Decomposition
    \\
    \vspace{4pt}
--------------------------- \ Final Exam \ ---------------------------
\end{itemize}
\end{frame}

\begin{frame}{Materials Recommended}
\begin{itemize}
    \item  Gilbert Strang, MIT 18.06, Linear Algebra (Spring 2005) \\
    \url{https://www.bilibili.com/video/BV1zx411g7gq}
    \item Essence of Linear Algebra \textit{-- by 3Blue1Brown} \\
    \url{https://www.bilibili.com/video/BV1ys411472E}
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{Gilbert.jpeg}
\end{figure}

\end{frame}

\begin{frame}{Further Study}
\textbf{When finishing this course...}
\vspace{5pt}

Gilbert Strang, MIT, A 2020 Vision of Linear Algebra (Spring 2020)
\url{https://www.bilibili.com/video/BV1GA411t7rL}
\vspace{5pt}

For those who will major in engineering, try these courses:
\begin{itemize}
    \item EE104 Fundamentals of Electric Circuits
    \item MA201b Ordinary Differential Equations B
    \item PHY203-15 Mathematical Methods in Physics
    \item CS405 Machine Learning
\end{itemize}

For those who will major in mathematics, try these courses:
\begin{itemize}
    \item MA109 Advanced Linear Algebra
    \item MA201a Ordinary Differential Equations A
\end{itemize}


\end{frame}

\section{The Geometry of Linear Equations}
\begin{frame}{Linear Equations}


\begin{examples}
Consider this linear equations system with 2 unknowns and 2 equations,
\begin{equation*}
    \begin{cases}
	2x-y=0\\
	-x+2y=3\\
\end{cases}
\end{equation*}
Solve the values of $x$, $y$.
\end{examples}

\textbf{Equivalent Matrix Form:}

\begin{equation*}
    \left[ \begin{matrix}
	2&		-1\\
	-1&		2\\
\end{matrix} \right] \left[ \begin{array}{c}
	x\\
	y\\
\end{array} \right] =\left[ \begin{array}{c}
	0\\
	3\\
\end{array} \right]
\end{equation*}

\begin{equation*}
    A\mathbf{x}=\mathbf{b}
\end{equation*}

Try to understand it in geometric way.
\end{frame}

\begin{frame}{Row Picture}
\vspace{-5pt}
\begin{equation*}
    \begin{cases}
	2x-y=0\\
	-x+2y=3\\
\end{cases}
\end{equation*}

Every row (equation) represents a single straight line.

The problem is to find the intersection of those straight lines.

\begin{figure}
    \centering
    \includegraphics[width=0.55\textwidth]{Row.jpg}
\end{figure}

\end{frame}

\begin{frame}{Column Picture}
\begin{columns}
\column{0.5\textwidth}
\begin{equation*}
    \begin{cases}
	2x-y=0\\
	-x+2y=3\\
\end{cases}
\end{equation*}

\column{0.5\textwidth}
\vspace{-9pt}
\begin{equation*}
    x\left[ \begin{array}{c}
	2\\
	-1\\
\end{array} \right] +y\left[ \begin{array}{c}
	-1\\
	2\\
\end{array} \right] =\left[ \begin{array}{c}
	0\\
	3\\
\end{array} \right]
\end{equation*}
\end{columns}

\vspace{4pt}
Every column (coefficients of a single variable) represents a vector.

The problem is to find the \textbf{Linear Combination} of those vectors.
\vspace{-1.03pt}

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{Column.jpg}
\end{figure}

\end{frame}

\section{Gaussian Elimination and Back-Substitution}
\begin{frame}{Gaussian Elimination}
The core problem of linear algebra is to solve linear equations.

Consider the following system of linear equations:

\begin{equation*}
    \begin{cases}
        x+2y+z=2\\
        3x+8y+z=12\\
        4y+z=2\\
    \end{cases}
\end{equation*}

Augmented matrix:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1&		2\\
        3&		8&		1&		12\\
        0&		4&		1&		2\\
    \end{matrix} \right]
\end{equation*}

How can we solve it?
\end{frame}

\begin{frame}{Gaussian Elimination}
Eliminate entry on position $(2,1)$:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1&		2\\
        0&		2&		-2&		6\\
        0&		4&		1&		2\\
    \end{matrix} \right]
\end{equation*}

Eliminate entry on position $(3,1)$:
\begin{equation*}
    (0, skip \: this \: step)
\end{equation*}

Eliminate entry on position $(3,2)$:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1&		2\\
        0&		2&		-2&		6\\
        0&		0&		5&		-10\\
    \end{matrix} \right]
\end{equation*}

Elimination process done! Find the pivots above.
\end{frame}


\begin{frame}{Back-Substitution}
After elimination, we get
\begin{columns}
    \column{0.5\textwidth}
    \begin{equation*}
        \left[ \begin{matrix}
            1&		2&		1&		2\\
            0&		2&		-2&		6\\
            0&		0&		5&		-10\\
        \end{matrix} \right]
    \end{equation*}

    \column{0.5\textwidth}
    \begin{equation*}
        \begin{cases}
            x+2y+z=2\\
            2y-2z=6\\
            5z=-10\\
        \end{cases}
    \end{equation*}
\end{columns}
Do back-substitution, we can get the solution:
\begin{equation*}
    \begin{cases}
        x=2\\
        y=1\\
        z=-2\\
    \end{cases}
\end{equation*}

Rethink the whole elimination process, are there exist some cases to let the elimination process break down?
\end{frame}

\begin{frame}{Singular Cases for Gauss Elimination}
The matrix we used above is a "good" matrix.
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1&		2\\
        3&		8&		1&		12\\
        0&		4&		1&		2\\
    \end{matrix} \right]
\end{equation*}

If we slightly change one of the element\dots
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1&		2\\
        3&		6&		1&		12\\
        0&		4&		1&		2\\
    \end{matrix} \right]
\end{equation*}

What will happen now?
\end{frame}

\begin{frame}{Singular Cases for Gauss Elimination}
Eliminate entry on position $(2,1)$:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1&		2\\
        0&		0&		-2&		6\\
        0&		4&		1&		2\\
    \end{matrix} \right]
\end{equation*}

Eliminate entry on position $(3,1)$:
\begin{equation*}
    (0, skip \: this \: step)
\end{equation*}

Eliminate entry on position $(3,2)$:
\begin{equation*}
    Something \: strange \: happens.
\end{equation*}

Elimination process fails! Row exchange to fix.

\end{frame}

\begin{frame}{Singular Cases for Gauss Elimination}
Another change to make the "good" matrix singular:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1&		2\\
        3&		8&		1&		12\\
        0&		4&		-4&		2\\
    \end{matrix} \right]
\end{equation*}

Eliminate entry on position $(2,1)$:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1&		2\\
        0&		2&		-2&		6\\
        0&		4&		-4&		2\\
    \end{matrix} \right]
\end{equation*}

Eliminate entry on position $(3,2)$:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1&		2\\
        0&		2&		-2&		6\\
        0&		0&		0&		-10\\
    \end{matrix} \right]
\end{equation*}

Elimination process fails! Missing pivots. No method to fix.

\end{frame}

\begin{frame}{Singular Cases for Gauss Elimination}
Two kind of singular cases that can let Gauss elimination break down:
\begin{itemize}
    \item Temporal failure: Can be fixed by row exchange.
    \item Permanent failure: Missing pivots. No method to fix.
\end{itemize}

\vspace{7pt}
Recall the problem you meet in assignment:

\vspace{5pt}
Which three numbers of $k$ does elimination break down?
\begin{equation*}
    \begin{cases}
        kx+3y=6\\
        3x+ky=-6\\
    \end{cases}
\end{equation*}

$k=0$ laeds to temporal failure, while $k=3\:or\:k=-3$ leads to permanent failure.

\end{frame}

\section{Matrix Multiplication}
\begin{frame}{Matrix Size in Matrix Multiplication}
Matrix $A$: $(m \times n)$, Matrix $B$: $(n \times p)$.

\vspace{3pt}
The size of $AB$: $(m \times p)$.

\vspace{3pt}
Consider the following multiplications, are they legal?

\begin{equation*}
    \left[ \begin{array}{c}
        1\\
        2\\
        5\\
    \end{array} \right] \left[ \begin{matrix}
        1&		2&		5\\
    \end{matrix} \right] , \left[ \begin{matrix}
        1&		2&		5\\
    \end{matrix} \right] \left[ \begin{array}{c}
        1\\
        2\\
        5\\
    \end{array} \right] , \left[ \begin{matrix}
        1&		2\\
        3&		3\\
        2&		1\\
    \end{matrix} \right] \left[ \begin{array}{c}
        1\\
        2\\
        3\\
    \end{array} \right]
\end{equation*}

\begin{equation*}
    \left[ \begin{matrix}
        1&		2\\
        3&		3\\
        2&		1\\
    \end{matrix} \right] \left[ \begin{array}{c}
        3\\
        7\\
    \end{array} \right] , \left[ \begin{matrix}
        3&		3\\
        5&		4\\
    \end{matrix} \right] \left[ \begin{matrix}
        4&		5&		6&		2\\
        3&		2&		4&		3\\
    \end{matrix} \right], \left[ 5 \right] \left[ \begin{array}{c}
        3\\
        2\\
        1\\
    \end{array} \right]
\end{equation*}

The size of the multiplication result will be...

Now, I will introduce 4 methods to calculate matrix multiplication.
\end{frame}

\begin{frame}{Matrix Multiplication - 4 Methods}
\begin{examples}
Compute the matrix multiplication.
    \begin{equation*}
        \left[ \begin{matrix}
            3&		-1&       1\\
            1&		 0&       1\\
            0&		 1&       2\\
        \end{matrix} \right] \left[ \begin{matrix}
            1&		 0&       1\\
            1&		-1&      -1\\
            1&		 2&       1\\
        \end{matrix} \right] =\left[ \begin{matrix}
            3&		 3&       5\\
            2&		 2&       2\\
            3&		 3&       1\\
        \end{matrix} \right]
    \end{equation*}
\end{examples}




\begin{enumerate}
    \item The regular (row-col) way. (Row of A) multiply (Column of B).
    \item The row way. Linear combination of (Row of B).
    \item The column way. Linear combination of (Column of A).
    \item The col-row way. (Column of A) multiply (Row of B).
\end{enumerate}

There is also an additional method: block method.

\vspace{3pt}
When you calculate matrix multiplication in the exam, please remember to choose another method for verification.
\end{frame}

\section{LU Factorization}
\begin{frame}{Elimination Matrices}
Now it's time to understand elimination matrices. The goal is to express the elimination process by matrix language.

\vspace{3pt}
For example, to eliminate $(2,1)$ position element:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1\\
        3&		8&		1\\
        0&		4&		1\\
    \end{matrix} \right] \rightarrow \left[ \begin{matrix}
        1&		2&		1\\
        0&		2&		-2\\
        0&		4&		1\\
    \end{matrix} \right]
\end{equation*}

What we have done: (Row 2) - 3 (Row 1).

\vspace{3pt}
Use a elimination matrix $E_{21}$ to represent this process:

\begin{equation*}
    \left[ \begin{matrix}
        1&		0&		0\\
        -3&		1&		0\\
        0&		0&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		2&		1\\
        3&		8&		1\\
        0&		4&		1\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		2&		1\\
        0&		2&		-2\\
        0&		4&		1\\
    \end{matrix} \right]
\end{equation*}

\begin{equation*}
    E_{21}A=A'
\end{equation*}

As you might discover, $E_{32}E_{31}E_{21}A=U$.
\end{frame}

\begin{frame}{Gauss Elimination Represented in Elimination Matrices}
The process of Gauss Elimination we introduced before:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1\\
        3&		8&		1\\
        0&		4&		1\\
    \end{matrix} \right] \xrightarrow{r2-3r1}\left[ \begin{matrix}
        1&		2&		1\\
        0&		2&		-2\\
        0&		4&		1\\
    \end{matrix} \right] \rightarrow \left[ \begin{matrix}
        1&		2&		1\\
        0&		2&		-2\\
        0&		4&		1\\
    \end{matrix} \right] \xrightarrow{r3-2r2}\left[ \begin{matrix}
        1&		2&		1\\
        0&		2&		-2\\
        0&		0&		5\\
    \end{matrix} \right]
\end{equation*}

Gauss Elimination represented in multipling elimination matrices:
\begin{equation*}
    \left[ \begin{matrix}
        1&		0&		0\\
        0&		1&		0\\
        0&		-2&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		0&		0\\
        0&		1&		0\\
        0&		0&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		0&		0\\
        -3&		1&		0\\
        0&		0&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		2&		1\\
        3&		8&		1\\
        0&		4&		1\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		2&		1\\
        0&		2&		-2\\
        0&		0&		5\\
    \end{matrix} \right]
\end{equation*}

We define $E=E_{32}E_{31}E_{21}$, then we can get $EA=U$.

\vspace{3pt}
Finally we can get $A=LU$, $L=E^{-1}={E_{21}}^{-1}{E_{31}}^{-1}{E_{32}}^{-1}$.

\vspace{3pt}
Remind that $E_{21},E_{31},...,L$ are all lower triangular matrices.
\end{frame}

\begin{frame}{Why $A=LU$, not $EA=U$ ?}
Have you ever thought about why we need to do $LU$ factorization instead of finding the overall elimination matrix $E$?

A simple comparison using the example above:

\begin{equation*}
    E=\left[ \begin{matrix}
        1&		0&		0\\
        0&		1&		0\\
        0&		-2&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		0&		0\\
        0&		1&		0\\
        0&		0&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		0&		0\\
        -3&		1&		0\\
        0&		0&		1\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		0&		0\\
        -3&		1&		0\\
        6&		-2&		1\\
    \end{matrix} \right]
\end{equation*}
\begin{equation*}
    L=\left[ \begin{matrix}
        1&		0&		0\\
        3&		1&		0\\
        0&		0&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		0&		0\\
        0&		1&		0\\
        0&		0&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		0&		0\\
        0&		1&		0\\
        0&		2&		1\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		0&		0\\
        3&		1&		0\\
        0&		2&		1\\
    \end{matrix} \right]
\end{equation*}

Computing $E$ is a quite complicated thing, on the contrary, when computing $L$, we just need to fill the blank and do not need any kinds of matrix multiplication!
\end{frame}

\begin{frame}{LU Factorization}
\begin{examples}
    Do the $LU$ factorization for matrix $A$:
    \begin{equation*}
        A=\left[ \begin{matrix}
            1&		1&		1\\
            1&		4&		4\\
            1&		4&		8\\
        \end{matrix} \right]
    \end{equation*}
\end{examples}

\textbf{Solution:}
\begin{equation*}
    \left[ \begin{matrix}
        1&		1&		1\\
        1&		4&		4\\
        1&		4&		8\\
    \end{matrix} \right] \xrightarrow{l_{21}=1}\left[ \begin{matrix}
        1&		1&		1\\
        0&		3&		3\\
        1&		4&		8\\
    \end{matrix} \right] \xrightarrow{l_{31}=1}\left[ \begin{matrix}
        1&		1&		1\\
        0&		3&		3\\
        0&		3&		7\\
    \end{matrix} \right] \xrightarrow{l_{32}=1}\left[ \begin{matrix}
        1&		1&		1\\
        0&		3&		3\\
        0&		0&		4\\
    \end{matrix} \right]
\end{equation*}

\begin{equation*}
    A=\left[ \begin{matrix}
        1&		1&		1\\
        1&		4&		4\\
        1&		4&		8\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		0&		0\\
        1&		1&		0\\
        1&		1&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		1&		1\\
        0&		3&		3\\
        0&		0&		4\\
    \end{matrix} \right]=LU
\end{equation*}
\end{frame}

\begin{frame}{LDU Factorization}
For $A$ above, continue to do LDU Factorization. That is to factor $U$ into $DU$.
One thing to remember is that $D$ is made by the pivots on the diagonal. So for the matrix $U$ above,
\begin{equation*}
    D=\left[ \begin{matrix}
        1&		0&		0\\
        0&		3&		0\\
        0&		0&		4\\
    \end{matrix} \right]
\end{equation*}

We do row operations to get matrix $L$, then we need to do column operations to get matrix $U$. The goal of this step is to do column operations to eliminate entries not on the diagonal.
\begin{equation*}
    \left[ \begin{matrix}
        1&		1&		1\\
        0&		3&		3\\
        0&		0&		4\\
    \end{matrix} \right] \xrightarrow[u_{12}=1]{c2-c1}\left[ \begin{matrix}
        1&		0&		1\\
        0&		3&		3\\
        0&		0&		4\\
    \end{matrix} \right] \xrightarrow[u_{13}=1]{c3-c1}\left[ \begin{matrix}
        1&		0&		0\\
        0&		3&		3\\
        0&		0&		4\\
    \end{matrix} \right] \xrightarrow[u_{23}=1]{c3-c2}\left[ \begin{matrix}
        1&		0&		0\\
        0&		3&		0\\
        0&		0&		4\\
    \end{matrix} \right]
\end{equation*}
\vspace{-1pt}
\begin{equation*}
    A=\left[ \begin{matrix}
        1&		1&		1\\
        1&		4&		4\\
        1&		4&		8\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		0&		0\\
        1&		1&		0\\
        1&		1&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		0&		0\\
        0&		3&		0\\
        0&		0&		4\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		1&		1\\
        0&		1&		1\\
        0&		0&		1\\
    \end{matrix} \right] =LDU=LDL^T
\end{equation*}

\end{frame}

\section{Row Exchanges and PA=LU Factorization}

\begin{frame}{Singular Cases for Gauss Elimination}
Recall that there are 2 singular cases when using the method of Gauss elimination, they can not be expressed as $A=LU$ factorization! But, the temporal failure can be fixed by row exchanges.

\vspace{3pt}
Do Gauss elimination (with the help of row exchanges) for the matrix below:

\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1&		2\\
        3&		6&		1&		12\\
        0&		4&		1&		2\\
    \end{matrix} \right] \rightarrow \left[ \begin{matrix}
        1&		2&		1&		2\\
        0&		0&		-2&		6\\
        0&		4&		1&		2\\
    \end{matrix} \right] \rightarrow \left[ \begin{matrix}
        1&		2&		1&		2\\
        0&		4&		1&		2\\
        0&		0&		-2&		6\\
    \end{matrix} \right]
\end{equation*}

Write the elimination matrices:

\begin{equation*}
    \left[ \begin{matrix}
        1&		0&		0\\
        -3&		1&		0\\
        0&		0&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		2&		1\\
        3&		6&		1\\
        0&		4&		1\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		2&		1\\
        0&		0&		-2\\
        0&		4&		1\\
    \end{matrix} \right]
\end{equation*}

Review: Which rule for matrix multiplication can help you understand the row operation above?

\end{frame}

\begin{frame}{Row Exchanges in Permutation Matrix}
Step 2 is to exchange Row 2 and Row 3, still using the row method for matrix multiplication, which matrix can express the process of row exchanges?

\begin{equation*}
    A\left[ \begin{matrix}
        1&		2&		1\\
        0&		0&		-2\\
        0&		4&		1\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		2&		1\\
        0&		4&		1\\
        0&		0&		-2\\
    \end{matrix} \right]
\end{equation*}

This matrix is to exchange the rows, so we call it permutation matrix $P$.

\begin{equation*}
    P\left[ \begin{matrix}
        1&		2&		1\\
        0&		0&		-2\\
        0&		4&		1\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		2&		1\\
        0&		4&		1\\
        0&		0&		-2\\
    \end{matrix} \right]
\end{equation*}

\end{frame}

\begin{frame}{Row Exchanges in Permutation Matrix}
The permutation matrix $P$:

\begin{equation*}
    P=\left[ \begin{matrix}
        1&		0&		0\\
        0&		0&		1\\
        0&		1&		0\\
    \end{matrix} \right]
\end{equation*}

\begin{itemize}
    \item For "1" at position $(1,1)$: (Row 1) $\rightarrow$ (Row 1).
    \item For "1" at position $(2,3)$: (Row 3) $\rightarrow$ (Row 2).
    \item For "1" at position $(3,2)$: (Row 2) $\rightarrow$ (Row 3).
\end{itemize}

One of the properties of permutation matrix $P$ is that it can only have one "1" in each row and column. Why?

\vspace{3pt}
After row exchanges, no rows before will disappear and no rows after will be all empty.
\end{frame}

\begin{frame}{Why $P^T=P^{-1}$?}
Why $P^T=P^{-1}$? A direct explanation:
\vspace{3pt}

\begin{columns}
\column{0.5\textwidth}
\begin{equation*}
    P=\left[ \begin{matrix}
        0&		1&		0&		0\\
        0&		0&		1&		0\\
        0&		0&		0&		1\\
        1&		0&		0&		0\\
    \end{matrix} \right]
\end{equation*}

\begin{itemize}
    \item 1 at $(1,2)$: (Row 2) $\rightarrow$ (Row 1).
    \item 1 at $(2,3)$: (Row 3) $\rightarrow$ (Row 2).
    \item 1 at $(3,4)$: (Row 4) $\rightarrow$ (Row 3).
    \item 1 at $(4,1)$: (Row 1) $\rightarrow$ (Row 4).
\end{itemize}

\column{0.5\textwidth}
\begin{equation*}
    P^T=\left[ \begin{matrix}
        0&		0&		0&		1\\
        1&		0&		0&		0\\
        0&		1&		0&		0\\
        0&		0&		1&		0\\
    \end{matrix} \right]
\end{equation*}

\begin{itemize}
    \item 1 at $(2,1)$: (Row 1) $\rightarrow$ (Row 2).
    \item 1 at $(3,2)$: (Row 2) $\rightarrow$ (Row 3).
    \item 1 at $(4,3)$: (Row 3) $\rightarrow$ (Row 4).
    \item 1 at $(1,4)$: (Row 4) $\rightarrow$ (Row 1).
\end{itemize}
\end{columns}

\vspace{7pt}
Exchange A and B then exchange B and A will result in exchange nothing.

\vspace{3pt}
Start to feel the beauty of multipling matrices by row?
\end{frame}


\begin{frame}{Expressing Row Exchanges using $PA=LU$}
The process of Gauss elimination and row exchanges can be expressed as:
\begin{equation*}
    \left[ \begin{matrix}
        1&		0&		0\\
        0&		0&		1\\
        0&		1&		0\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		0&		0\\
        -3&		1&		0\\
        0&		0&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		2&		1\\
        3&		6&		1\\
        0&		4&		1\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		2&		1\\
        0&		4&		1\\
        0&		0&		-2\\
    \end{matrix} \right]
\end{equation*}

\begin{equation*}
    PE_{21}A=U
\end{equation*}

Can we do row exchanges at the beginning?
\begin{equation*}
    EPA=U
\end{equation*}

Then, left multiply $E^{-1}$ on both sides:
\begin{equation*}
    PA=LU
\end{equation*}
\end{frame}

\begin{frame}{$PA=LU$ Factorization}
\begin{example}
    Do $PA=LU$ factorization for matrix $A$:
    \begin{equation*}
        A=\left[ \begin{matrix}
            1&		2&		1\\
            3&		6&		1\\
            0&		4&		1\\
        \end{matrix} \right]
    \end{equation*}
\end{example}
\textbf{Solution:}
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1\\
        3&		6&		1\\
        0&		4&		1\\
    \end{matrix} \right] \xrightarrow[l_{21}=3]{r2-3r1}\left[ \begin{matrix}
        1&		2&		1\\
        0&		0&		-2\\
        0&		4&		1\\
    \end{matrix} \right] \xrightarrow{r2\leftrightarrow r3}\left[ \begin{matrix}
        1&		2&		1\\
        0&		4&		1\\
        0&		0&		-2\\
    \end{matrix} \right]
\end{equation*}

Restart the process and do row exchanges firstly.
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1\\
        3&		6&		1\\
        0&		4&		1\\
    \end{matrix} \right] \xrightarrow{r2\leftrightarrow r3}\left[ \begin{matrix}
        1&		2&		1\\
        0&		4&		1\\
        3&		6&		1\\
    \end{matrix} \right] \xrightarrow[l_{31}=3]{r3-3r1}\left[ \begin{matrix}
        1&		2&		1\\
        0&		4&		1\\
        0&		0&		-2\\
    \end{matrix} \right]
\end{equation*}
\end{frame}

\begin{frame}{$PA=LU$ Factorization}
\begin{equation*}
    \left[ \begin{matrix}
        1&		0&		0\\
        0&		0&		1\\
        0&		1&		0\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		2&		1\\
        3&		6&		1\\
        0&		4&		1\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		0&		0\\
        0&		1&		0\\
        3&		0&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		2&		1\\
        0&		4&		1\\
        0&		0&		-2\\
    \end{matrix} \right]
\end{equation*}

\begin{equation*}
    PA=LU
\end{equation*}

Remember to verify the answer! Do 2 multiplications and check if they are the same.

Suppose we do not do row exchanges at first, what will happen\dots

\begin{equation*}
    \left[ \begin{matrix}
        1&		0&		0\\
        0&		0&		1\\
        0&		1&		0\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		2&		1\\
        3&		6&		1\\
        0&		4&		1\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		0&		0\\
        3&		1&		0\\
        0&		0&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		2&		1\\
        0&		4&		1\\
        0&		0&		-2\\
    \end{matrix} \right] ???
\end{equation*}

So, if you find a matrix need to have row operations, please restart and do them first. That is the only way to avoid mistakes!
\end{frame}

\section{Inverse of Matrix}
\begin{frame}{Definition of Inverse}
\begin{theorem}
An $n \times n$ matrix $A$ is said to be invertible if there is an $n \times n$ matrix $B$ such that
\begin{equation*}
    AB=BA=I
\end{equation*}
In this case, $B$ is called an \alert{inverse} of $A$.
\end{theorem}

Have you ever thought about the non-square cases? Can non-square matrices have inverse?

No, matrix size determines that! But...
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1\\
        0&		2&		1\\
    \end{matrix} \right] \left[ \begin{matrix}
        1&		-1\\
        0&		-1\\
        0&		3\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		0\\
        0&		1\\
    \end{matrix} \right]
\end{equation*}

Well, non-square matrices may have left inverses and right inverses.
\end{frame}

\begin{frame}{Existence of Inverse}
A very important problem is: does a particular matrix have an inverse?

\vspace{3pt}
I say that the matrix $A$ below cannot have an inverse:
\begin{equation*}
    A=\left[ \begin{matrix}
        1&		2\\
        3&		6\\
    \end{matrix} \right]
\end{equation*}
Who can give me a reason to let me believe that?

If you have learnt something about determinant, you can find that
\begin{equation*}
    \left| \begin{matrix}
        1&		2\\
        3&		6\\
    \end{matrix} \right|=6-6=0
\end{equation*}

The determinant is 0 so that the matrix is not invertible.

\vspace{3pt}
Can we use the knowledge in Chapter 1 to explain it?
\end{frame}

\begin{frame}{Existence of Inverse}
According to definition, if it has an inverse, there exist a matrix $B$ to let $AB=I$, thus, $B$ is the inverse of $A$.
\begin{equation*}
    \left[ \begin{matrix}
        1&		2\\
        3&		6\\
    \end{matrix} \right] \left[ \begin{matrix}
        x&		\cdot\\
        y&		\cdot\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		0\\
        0&		1\\
    \end{matrix} \right]
\end{equation*}

Recall the column method of matrix multiplication:
\begin{equation*}
    x\left[ \begin{array}{c}
        1\\
        3\\
    \end{array} \right] +y\left[ \begin{array}{c}
        2\\
        6\\
    \end{array} \right] =\left[ \begin{array}{c}
        1\\
        0\\
    \end{array} \right]
\end{equation*}

It may lead you to think about column picture.

\vspace{3pt}
Is it possible? Linear combinations of the 2 column vectors are still on the straight line!

\vspace{3pt}
Therefore, the inverse cannot exist for this matrix $A$.
\end{frame}

\begin{frame}{Existence of Inverse}
I have to say I prefer to use another interpretation to explain that.

\begin{theorem}
Suppose there is a nonzero vector $\mathbf{x}$ such that $A\mathbf{x}=0$, then $A$ cannot have an inverse.
\end{theorem}

Maybe you all know that before, but why?

For this matrix $A$, find a nonzero solution for $A\mathbf{x}=0$:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2\\
        3&		6\\
    \end{matrix} \right] \left[ \begin{array}{c}
        2\\
        -1\\
    \end{array} \right] =\left[ \begin{array}{c}
        0\\
        0\\
    \end{array} \right]
\end{equation*}

Suppose there is an inverse to make $A^{-1}A=I$, multiply $A^{-1}$ on both sides:
\begin{equation*}
    A\mathbf{x}=0\rightarrow A^{-1}A\mathbf{x}=A^{-1}0\rightarrow \mathbf{x}=0
\end{equation*}

Well, but there is a nonzero solution (2,-1) for matrix $A$! So matrix $A$ cannot have an inverse.
\end{frame}

\begin{frame}{Calculation of Inverse}
Another important problem: how to find the inverse $A^{-1}$ for invertible matrix $A$?

\vspace{3pt}
Gauss-Jordan method, right? But how does it come?

\vspace{3pt}
You may think up with this method:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2\\
        3&		7\\
    \end{matrix} \right] \left[ \begin{matrix}
        a&		c\\
        b&		d\\
    \end{matrix} \right] =\left[ \begin{matrix}
        1&		0\\
        0&		1\\
    \end{matrix} \right]
\end{equation*}

To find the inverse, we only need to find the 4 unknowns $a,b,c,d$.

\vspace{3pt}
Use the column method of matrix multiplication to think, actually we only need to find the solutions for the following 2 linear equation systems:

\vspace{6pt}
\begin{columns}
    \column{0.5\textwidth}
    \begin{equation*}
        \left[ \begin{matrix}
            1&		2\\
            3&		7\\
        \end{matrix} \right] \left[ \begin{array}{c}
            a\\
            b\\
        \end{array} \right] =\left[ \begin{array}{c}
            1\\
            0\\
        \end{array} \right]
    \end{equation*}

    \column{0.5\textwidth}
    \begin{equation*}
        \left[ \begin{matrix}
            1&		2\\
            3&		7\\
        \end{matrix} \right] \left[ \begin{array}{c}
            c\\
            d\\
        \end{array} \right] =\left[ \begin{array}{c}
            0\\
            1\\
        \end{array} \right]
    \end{equation*}
\end{columns}

\vspace{6pt}
Then, the problem returns to Gauss elimination!
\end{frame}

\begin{frame}{Calculation of Inverse}
Let's solve one of the systems together.
\begin{equation*}
    \left[ \begin{matrix}
        1&		2\\
        3&		7\\
    \end{matrix} \right] \left[ \begin{array}{c}
        a\\
        b\\
    \end{array} \right] =\left[ \begin{array}{c}
        1\\
        0\\
    \end{array} \right]
\end{equation*}

Augmented matrix:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1\\
        3&		7&		0\\
    \end{matrix} \right]
\end{equation*}

Solve this equation system by Gauss elimination:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1\\
        3&		7&		0\\
    \end{matrix} \right] \rightarrow \left[ \begin{matrix}
        1&		2&		1\\
        0&		1&		-3\\
    \end{matrix} \right] \rightarrow \left[ \begin{matrix}
        1&		0&		\mathbf{7}\\
        0&		1&		\mathbf{-3}\\
    \end{matrix} \right]
\end{equation*}

The solution is $\left[ \begin{array}{c}
	a\\
	b\\
\end{array} \right] =\left[ \begin{array}{c}
	7\\
	-3\\
\end{array} \right] $ as the last column displays.

\vspace{3pt}
Then we calculate another equation system\dots


\end{frame}

\begin{frame}{Calculation of Inverse}
At this time, Jordan appears, he said to Gauss: "Why not solve 2 equations at once?"

\vspace{3pt}
The total augmented matrix is:
\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1&		0\\
        3&		7&		0&		1\\
    \end{matrix} \right]
\end{equation*}

Then, if we eliminate the coefficient to $I$, the solution of 2 equation systems (inverse of $A$) will appear in the last 2 columns!

\begin{equation*}
    \left[ \begin{matrix}
        1&		2&		1&		0\\
        3&		7&		0&		1\\
    \end{matrix} \right] \rightarrow \left[ \begin{matrix}
        1&		2&		1&		0\\
        0&		1&		-3&		1\\
    \end{matrix} \right] \rightarrow \left[ \begin{matrix}
        1&		0&		7&		-2\\
        0&		1&		-3&		1\\
    \end{matrix} \right]
\end{equation*}

So, we can finally get that
\begin{equation*}
    A^{-1}=\left[ \begin{matrix}
        7&		-2\\
        -3&		1\\
    \end{matrix} \right]
\end{equation*}

That is the Gauss-Jordan method, and I believe you can have a better understanding on it now.
\end{frame}

\begin{frame}{Additional: Block Method to Find the Inverse}
\begin{example}
    Find the inverse of this matrix, given that $A,B,C$ are all invertible matrices. Please express the result in matrix form.
    \begin{equation*}
        L=\left[ \begin{matrix}
            A&		0\\
            B&		C\\
        \end{matrix} \right],\:
        M=\left[ \begin{matrix}
            0&		A\\
            B&		0\\
        \end{matrix} \right]
    \end{equation*}
\end{example}

\textbf{Solution:}

Core idea: Treat all block matrices as a single entry.
\begin{equation*}
    \left[ \begin{matrix}
        A&		0&		I&		0\\
        B&		C&		0&		I\\
    \end{matrix} \right] \rightarrow \left[ \begin{matrix}
        A&		0&		I&		0\\
        0&		C&		-BA^{-1}&		I\\
    \end{matrix} \right] \rightarrow \left[ \begin{matrix}
        I&		0&		A^{-1}&		0\\
        0&		I&		-C^{-1}BA^{-1}&		C^{-1}\\
    \end{matrix} \right]
\end{equation*}

\begin{equation*}
    L^{-1}=\left[ \begin{matrix}
        A^{-1}&		0\\
        -C^{-1}BA^{-1}&		C^{-1}\\
    \end{matrix} \right]
\end{equation*}

\end{frame}

\section{Vector Spaces and Subspaces}
\begin{frame}{Vector Spaces}
Once the vector in a space can have addition and scalar multiplication, and the result is still in this space, then the space can be called vector spaces.

Some standard vector spaces:
\begin{itemize}
    \item $\mathbb{R}^1$: x axis. 1 dimensional.
    \item $\mathbb{R}^2$: x-y plane. 2 dimensional.
    \item $\mathbb{R}^3$: x-y-z space. 3 dimensional.
\end{itemize}

\begin{example}
    Are these sets below vector spaces? Explain it.
    \begin{enumerate}
        \item $\mathbb{R}^3$ space without the origin.
        \item $\mathbb{R}^2$ plane without the third quadrant.
        \item $\mathbb{R}^{2021}$ space.
        \item $\mathbb{C}^2$ space.
    \end{enumerate}
\end{example}
\end{frame}

\begin{frame}{Subspaces}
\begin{definition}
    If $S$ in a nonempty subset of a vector space $V$, and $S$ satisfies the conditions: linear combinations stay in the subspace.
    \begin{itemize}
        \item $c\mathbf{u}\in S$ whenever $\mathbf{u}\in S$ for any scalar c
        \item $\mathbf{u} + \mathbf{v}\in S$ whenever $\mathbf{u}\in S$ and $\mathbf{v}\in S$
    \end{itemize}
    Then $S$ is said to be a subspace of $V$.
\end{definition}

To say it in human language, if a space $S$ is a closed under linear combinations and it is in a vector space $V$, then $S$ is a subspace of $V$.

\vspace{3pt}
Remember that the origin should exist in every subspace or vector space!

\vspace{3pt}
It is an abstract concept but no need to worry about that. You will understand it later.
\end{frame}
\end{document}